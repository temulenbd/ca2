{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d04671",
   "metadata": {},
   "source": [
    "                                                                       TEMUULEN Bulgan - 2022427"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf25cb4",
   "metadata": {},
   "source": [
    "### CCT College Dublin Continuous Assessment No.2\n",
    "# AN ANALYSIS OF INDIAN FARMERS' PROTEST TWEETS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad4ca0",
   "metadata": {},
   "source": [
    "**Brief Introduction**\n",
    "\n",
    "1. For my second continuous assessment, I choose CSV format data of Indian Farmer's Protest Tweets. This file contains over 1 million English language tweets tweeted between November 1st, 2020 and november 21st, 2021 with the hashtag <#FarmersProtest>. It is downloaded from the Kaggle website with the CCO:Public Domain license.\n",
    "\n",
    "2. I divided my project into 3 primary sections (Every step in data processing and analysis is fully discussed on each subsection of these primary sections.):\n",
    "    1. big data storage and processing\n",
    "    2. comparative analysis of databases\n",
    "    3. sentiment analysis and forecast\n",
    "\n",
    "3. I used Git for daily code tracking and GitHub for archiving, monitoring and sharing. To view the whole project on GitHub, click on the follwig link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d4f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, importing libraries that I used in this project.\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de876cf9",
   "metadata": {},
   "source": [
    "## 1. Big Data Storage and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80edbb",
   "metadata": {},
   "source": [
    "Since the size of the data I choose is 1.8Â GB it would be difficult for me to upload and process it on my VM, I decided to do some preprocessing in Python's Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47471a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a variable with file size information.\n",
    "file_size = os.path.getsize('/home/hduser/Desktop/ca/tweets.csv')/(1024*1024*1024)\n",
    "\n",
    "# Printing variable value to check the size.\n",
    "print(\"The size of the tweet file is \", round(file_size, 2), 'GB.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b503dd",
   "metadata": {},
   "source": [
    "### 1.1. Dataset prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame.\n",
    "df_preprocess = pd.read_csv('/home/hduser/Desktop/ca/tweets.csv')\n",
    "\n",
    "# Checking the DataFrame.\n",
    "df_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fc0c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Examaning DataFrame.\n",
    "df_preprocess.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dfac9",
   "metadata": {},
   "source": [
    "This dataset contains 8 distinct categories of information that I will use for further analysis. Rest is not important, it will use only memory and space of my VM, that's why i'm going to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac869a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting columns from the DataFrame\n",
    "#df_preprocess = df_preprocess.drop(labels=['tweetUrl', 'source', 'media', 'retweetedTweet', 'quotedTweet', 'mentionedUsers'],\n",
    "                                   axis=1)\n",
    "\n",
    "# Checking the changes.\n",
    "df_preprocess.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84dc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new file of tweets on my VM with the utf-8 Encoding.\n",
    "#df_preprocess.to_csv('/home/hduser/Desktop/ca/new_tweets.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2485787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable with file size information.\n",
    "file_size = os.path.getsize('/home/hduser/Desktop/ca/new_tweets.csv')/(1024*1024)\n",
    "\n",
    "# Printing variable value to check the size.\n",
    "print(\"The size of the tweet file is \", round(file_size, 2), 'MB.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9a18c",
   "metadata": {},
   "source": [
    "Now the file size is reduced to 272.8 and we still have importand information of the tweets for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5681470",
   "metadata": {},
   "source": [
    "### 1.2 *Processing the data in Pyspark.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd996e6",
   "metadata": {},
   "source": [
    "I've copied the 'new_tweet.csv' file from the local file system to HDFS's new created directory 'ca2'. And now I would like to do some processing job to the file using Pyspark library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9020fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a new Sparksession for data import from HDFS.\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('HDFS Data Import') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d5b1cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating a variable with \n",
    "spark_df = spark.read.csv('/ca2/new_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16155272",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+-------+------+----------+------------+---------+----------+\n",
      "|             _c0|                 _c1|                 _c2|    _c3|   _c4|       _c5|         _c6|      _c7|       _c8|\n",
      "+----------------+--------------------+--------------------+-------+------+----------+------------+---------+----------+\n",
      "|            null|                date|     renderedContent|tweetId|userId|replyCount|retweetCount|likeCount|quoteCount|\n",
      "|               0|2021-03-30 03:33:...|          Support ðŸ‘‡|   null|  null|      null|        null|     null|      null|\n",
      "|#FarmersProtest\"|1.376739399593910...|1.015969769760096...|      0|     0|         0|           0|     null|      null|\n",
      "+----------------+--------------------+--------------------+-------+------+----------+------------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "449e9948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string'),\n",
       " ('_c5', 'string'),\n",
       " ('_c6', 'string'),\n",
       " ('_c7', 'string'),\n",
       " ('_c8', 'string')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47933f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89bcbc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(type(spark_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc90f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
