{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d04671",
   "metadata": {},
   "source": [
    "                                                                       TEMUULEN Bulgan - 2022427"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf25cb4",
   "metadata": {},
   "source": [
    "### CCT College Dublin Continuous Assessment No.2\n",
    "# AN ANALYSIS OF INDIAN FARMERS' PROTEST TWEETS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad4ca0",
   "metadata": {},
   "source": [
    "**Brief Introduction of the project:**\n",
    "\n",
    "1. For my second continuous assessment, I choose CSV format data of Indian Farmer's Protest Tweets. This file contains over 1 million English language tweets tweeted between November 1st, 2020 and november 21st, 2021 with the hashtag <#FarmersProtest>. It is downloaded from the Kaggle website with the CCO:Public Domain license.\n",
    "(https://kaggle.com/datasets/prathamsharma123/farmers-protest-tweets-dataset-csv)\n",
    "\n",
    "2. I divided my project into 3 primary sections (Every step in data processing and analysis is fully discussed on each subsection of these primary sections.):\n",
    "    1. big data storage and processing\n",
    "    2. EDA\n",
    "    3. sentiment analysis and forecast\n",
    "    4. comparison of the databases\n",
    "\n",
    "3. I used Git for daily code tracking and GitHub for archiving, monitoring and sharing. To view the whole project on GitHub, click on the follwig link "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3b7ce",
   "metadata": {},
   "source": [
    "**Libraries and modules used for this project:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d4f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de876cf9",
   "metadata": {},
   "source": [
    "## 1. Big Data Storage and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80edbb",
   "metadata": {},
   "source": [
    "For my second continous project I choose Kaggle.com's dataset called 'Farmers Protest Tweets'. It was collected by the hashtag #FarmersProtest including CCO:Pulic Domain license which means that this dataset allows copy, modify, distribute and perform the work, even for commercial purpososes, all without asking permission. All the tweets in it in English Language and collected from Twitter.com from November 1st, 2020 to November 21st, 2021. The main subject matter of these tweets are about the biggest anti-farm laws protest which took place at the borders of the Indian natioanl capital of New Delhi, organized by coalition of over 40 farmer from across the country.The dataset extraction process was done by Pratham Sharma, kaggle datasets expert who used Twitter API and  snscrape Python library for collection. The tweets data consist of two separate CSVL files with size the size of 1.7¬†GB and 81.2 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47471a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the tweets file is: 1.69 GB.\n",
      "The size of the users file is: 81.18 MB.\n"
     ]
    }
   ],
   "source": [
    "# Checking the file size of the file.\n",
    "file_size = os.path.getsize('/home/hduser/Desktop/ca/tweets.csv')/(1024*1024*1024)\n",
    "print(f'The size of the tweets file is: {file_size:.2f} GB.')\n",
    "file_size = os.path.getsize('/home/hduser/Desktop/ca/users.csv')/(1024*1024)\n",
    "print(f'The size of the users file is: {file_size:.2f} MB.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b503dd",
   "metadata": {},
   "source": [
    "### 1.1. *Preprocessing in Python.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83926380",
   "metadata": {},
   "source": [
    "Before to start processing the DataSet in distributed file system platforms I decided to examine each CSV file using Pandas on Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53351e86",
   "metadata": {},
   "source": [
    "***tweets.csv***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9151b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame.\n",
    "df_tweets = pd.read_csv('/home/hduser/Desktop/ca/tweets.csv')\n",
    "\n",
    "# Checking the DataFrame.\n",
    "df_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fc0c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Printing information about the DataFrame\n",
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63926d09",
   "metadata": {},
   "source": [
    "***users.csv***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame.\n",
    "df_users = pd.read_csv('/home/hduser/Desktop/ca/users.csv')\n",
    "\n",
    "# Checking the DataFrame.\n",
    "df_users.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing information about the DataFrame\n",
    "df_users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dfac9",
   "metadata": {},
   "source": [
    "As we can see from abowe first two code sells the DataSet with tweets consisted of 13 distinct columns of information where some of which is not very importand for further processing. The columns such as 'tweetUrl', 'tweetId', 'source', 'media', 'retweetedTweet', 'quotedTweet', 'mentionedUsers', and 'userId' doesn't include importand information for analysis.\n",
    "\n",
    "On the other hand last two code sells show that the DataSet with users information consisted of 18 distincst columns of information from which I can only use only column with 'display name. So further I'm going to remove unnessecary columns from each DataFrame and merge them as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac869a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deleting columns from the DataFrame of tweets.\n",
    "df_tweets = df_tweets.drop(labels=['tweetUrl', 'tweetId', 'source', 'media', 'retweetedTweet', 'quotedTweet', \\\n",
    "                                   'mentionedUsers'], axis=1)\n",
    "\n",
    "# Checking the changes.\n",
    "df_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting columns from the DataFrame of tweets.\n",
    "df_users = df_users[['displayname', 'userId']]\n",
    "\n",
    "# Checking the changes.\n",
    "df_users.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026b755",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Performing left merging on two DataFrames.\n",
    "df_final = pd.merge(df_tweets, df_users, on='userId', how='left')\n",
    "\n",
    "# Deleting the column 'userId'.\n",
    "df_final = df_final.drop(labels=['userId'], axis=1)\n",
    "\n",
    "# Checking the changes.\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823631b",
   "metadata": {},
   "source": [
    "I merged two DataFrames and removed unessary columns. Now I'm going to save the it as a CSV file on my Ubuntu VM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as 'new_tweets.csv' on my VM with the utf-8 Encoding.\n",
    "df_final.to_csv('/home/hduser/Desktop/ca/new_tweets.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d44ce26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of new tweet file is: 670.19 MB.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m new_file_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/hduser/Desktop/ca/new_tweets.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe size of new tweet file is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_file_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_final\u001b[49m\u001b[38;5;241m.\u001b[39mshape())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_final' is not defined"
     ]
    }
   ],
   "source": [
    "# Checking the file size.\n",
    "new_file_size = os.path.getsize('/home/hduser/Desktop/ca/new_tweets.csv')/(1024*1024)\n",
    "print(f'The size of new tweet file is: {new_file_size:.2f} MB.')\n",
    "\n",
    "print(df_final.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11069657",
   "metadata": {},
   "source": [
    "The file size is reduced from 1.7GB to 670.2MB and it still have importand information of the tweets for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5681470",
   "metadata": {},
   "source": [
    "### 1.2 *Data cleaning in Pyspark.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058322bc",
   "metadata": {},
   "source": [
    "I desided to store my new created twitter dataset file in HDFS and before doing EDA and sentiment analysis I'll further do more thorough preprocessing and cleaning done using Apache Spark tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8babcb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a new SparkSession for data import from HDFS.\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('HDFS Data Import') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a896a58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading the file.\n",
    "df_spark = spark.read.option('header', 'true') \\\n",
    "                        .option('multiline', 'true') \\\n",
    "                        .option('quote', \"\\\"\") \\\n",
    "                        .option('escape', \"\\\"\") \\\n",
    "                        .csv('/ca2/new_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed840b",
   "metadata": {},
   "source": [
    "Using 'SparkSession.builder' I created a new SparkSession for interacting with Spark functionality. Then I created a new PySpark DataFrame by importing the 'new_tweets.csv' file which is stored in HDFS's 'ca2' directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbded4e4",
   "metadata": {},
   "source": [
    "***columns***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474eb494",
   "metadata": {},
   "source": [
    "First thing I would like to do is check all the columns and its datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "542642c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+------------+---------+----------+-----------------+\n",
      "|                date|     renderedContent|replyCount|retweetCount|likeCount|quoteCount|      displayname|\n",
      "+--------------------+--------------------+----------+------------+---------+----------+-----------------+\n",
      "|2021-03-30 03:33:...|Support üëá\\n\\n#Fa...|         0|           0|        0|         0|             null|\n",
      "|2021-03-30 03:33:...|Supporting farmer...|         0|           0|        0|         0|             null|\n",
      "|2021-03-30 03:31:...|Support farmers i...|         0|           0|        0|         0|             null|\n",
      "|2021-03-30 03:30:...|#StopHateAgainstF...|         0|           1|        3|         0|    Sukhdev Singh|\n",
      "|2021-03-30 03:30:...|You hate farmers ...|         0|           0|        1|         0|             null|\n",
      "|2021-03-30 03:29:...|They can't be far...|         0|           0|        0|         0|Abhimanyu üåè üáÆüá≥|\n",
      "|2021-03-30 03:29:...|They can't be far...|         0|           0|        0|         0|Abhimanyu üåè üáÆüá≥|\n",
      "|2021-03-30 03:28:...|Lets not forget t...|         0|           2|        3|         0|    Japneet Singh|\n",
      "|2021-03-30 03:28:...|@Troll48611422 @D...|         1|           0|        0|         0|             null|\n",
      "|2021-03-30 03:28:...|Neutrality helps ...|         0|           0|        0|         0|    Kisan Botüöúüåæ|\n",
      "+--------------------+--------------------+----------+------------+---------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the contents of the DataFrame.\n",
    "df_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8170170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of the DataFrame: date renderedContent replyCount retweetCount likeCount quoteCount displayname \n"
     ]
    }
   ],
   "source": [
    "# Checking the columns of the DataFrame\n",
    "x = 0\n",
    "columns = ''\n",
    "while x < len(df_spark.columns):\n",
    "    columns += df_spark.columns[x] + ' '\n",
    "    x += 1\n",
    "print('Columns of the DataFrame:', columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27485de3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- renderedContent: string (nullable = true)\n",
      " |-- replyCount: string (nullable = true)\n",
      " |-- retweetCount: string (nullable = true)\n",
      " |-- likeCount: string (nullable = true)\n",
      " |-- quoteCount: string (nullable = true)\n",
      " |-- displayname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking column dtypes.\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411ec44",
   "metadata": {},
   "source": [
    "I examined the DataFrame with its columns and found that because it was imported from the CSV file all 7 columns contained the string dtype values. I'll change the names of the columns to make it easy to use, and dtypes of the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6dfa491",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Renaming the columns.\n",
    "df_spark = df_spark.select(col('date').alias('date'),\n",
    "                           col('displayname').alias('user'),\n",
    "                           col('renderedContent').alias('tweet'),\n",
    "                           col('replyCount').alias('replied'),\n",
    "                           col('retweetCount').alias('retweeted'),\n",
    "                           col('likeCount').alias('liked'),\n",
    "                           col('quoteCount').alias('quoted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cbcbdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing dtypes.\n",
    "df_spark = df_spark.withColumn('date',to_date(col('date').cast(DateType())))\n",
    "df_spark = df_spark.withColumn('tweet',col('tweet').cast(StringType()))\n",
    "df_spark = df_spark.withColumn('user',col('user').cast(StringType()))\n",
    "df_spark = df_spark.withColumn('replied',col('replied').cast('integer'))\n",
    "df_spark = df_spark.withColumn('retweeted',col('retweeted').cast('integer'))\n",
    "df_spark = df_spark.withColumn('liked',col('liked').cast('integer'))\n",
    "df_spark = df_spark.withColumn('quoted',col('quoted').cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaef4dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------------------+-------+---------+-----+------+\n",
      "|      date|user|               tweet|replied|retweeted|liked|quoted|\n",
      "+----------+----+--------------------+-------+---------+-----+------+\n",
      "|2021-03-30|null|Support üëá\\n\\n#Fa...|      0|        0|    0|     0|\n",
      "|2021-03-30|null|Supporting farmer...|      0|        0|    0|     0|\n",
      "+----------+----+--------------------+-------+---------+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking changes.\n",
    "df_spark.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37eae29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- replied: integer (nullable = true)\n",
      " |-- retweeted: integer (nullable = true)\n",
      " |-- liked: integer (nullable = true)\n",
      " |-- quoted: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking changes.\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d9338",
   "metadata": {},
   "source": [
    "Now all the column values has the appropirate data format as well as the names of the columns are short and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e99f4",
   "metadata": {},
   "source": [
    "***null values and duplicates***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8581c03f",
   "metadata": {},
   "source": [
    "Second thing I would like to do for cleaning is removing the duplicates and null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b6c1ae8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2953850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Examining the shape of the DataFrame\n",
    "print('The DataFrame consists of', len(df_spark.columns), 'columns and', df_spark.count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826a47a",
   "metadata": {},
   "source": [
    "The Dataframe consists of 2.953.850 rows and 7 colums. And as can be seen from the 'show()' method above it is obvious that there are lots of duplicates on it, so I would like to remove them. \n",
    "\n",
    "Moreover, there are some NaN values in the new merged column 'displayname'. It seems like some users that tweeted on the twitter doesn't have a Display Name. It might indicate that they no longer user of the social media plaform, or even might have blocked becuse of trolling or whatever reason, the account might have been deleted. So to prevent the bias and also to not waste my memory for extra processing I have decided to drop the rows which doesn't contain the display name or contain duplicate tweet entries.\n",
    "\n",
    "After these changes I'll order table contents by the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6be6faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicates.\n",
    "df_spark = df_spark.drop_duplicates()\n",
    "\n",
    "# Dropping rows with null values in the column 'displayname'.\n",
    "df_spark = df_spark.filter(col('displayname').isNotNull())\n",
    "\n",
    "# Ordering by date.\n",
    "df_spark = df_spark.orderBy('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f83dd81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------+---------+-----+------+\n",
      "|      date|                user|               tweet|replied|retweeted|liked|quoted|\n",
      "+----------+--------------------+--------------------+-------+---------+-----+------+\n",
      "|2020-11-01|Manickam Tagore ....|Yesterday in a pu...|      8|       95|  389|     5|\n",
      "|2020-11-01|          Rajay Deep|Such a shame!\\n\\n...|      0|        0|    2|     0|\n",
      "|2020-11-01|          OnTheFritz|@WhiteHouse @real...|      1|        0|    3|     0|\n",
      "|2020-11-01|        ClaireDeLune|Has this been rep...|      0|        0|    1|     0|\n",
      "|2020-11-02|     pravingandhino1|.@asadowaisi \\nTh...|      0|        0|    0|     0|\n",
      "|2020-11-02|         Bimal Jaggi|#GujjarReservatio...|      0|        0|    1|     0|\n",
      "|2020-11-02|   Vishal Maheshwari|@narendramodi Ver...|      0|        0|    0|     0|\n",
      "|2020-11-02|IndoAsianCommodities|Given #FarmersPro...|      0|        0|    0|     0|\n",
      "|2020-11-02|Fateh Singh  Bhullar|Other side of APM...|      0|        0|    1|     0|\n",
      "|2020-11-03|         Rajan kumar|signature campaig...|      0|        0|    1|     0|\n",
      "+----------+--------------------+--------------------+-------+---------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking the changes.\n",
    "df_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b5ca8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame consists of 7 columns and 1066380 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking the changes.\n",
    "print('The DataFrame consists of', len(df_spark.columns), 'columns and', df_spark.count(), 'rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0bd9632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-------+---------+-----+------+\n",
      "|date|user|tweet|replied|retweeted|liked|quoted|\n",
      "+----+----+-----+-------+---------+-----+------+\n",
      "+----+----+-----+-------+---------+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking null values on entire DataFrame.\n",
    "df_spark.filter(col('date').isNull() | col('user').isNull() | col('tweet').isNull() | col('replied').isNull() | \\\n",
    "                col('retweeted').isNull() | col('liked').isNull() | col('quoted').isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b70e48",
   "metadata": {},
   "source": [
    "Above executed cells show that now the DataFrame shape is 1.066.380x7. There are ni dublicates and no null values, it is now clean and clear. However since I'll do the sentimental analysis for this project I would like to clean the entries of the column 'tweet'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff1c63",
   "metadata": {},
   "source": [
    "***the column 'tweet'***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e806e41",
   "metadata": {},
   "source": [
    "Firstly I will remove tags, hashtags, emails, links from the tweets because I would like to focus on the text itself, rather than considering the additional context. They extra contexts are not  directly contribute to the sentiment of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48261669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing tags, hashtags, emails, and website links from the values of the column 'tweet'.\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', r'@\\w+|#\\S+|\\S+@\\S+|http\\S+|www\\S+|\\S+/\\S+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "670e2b3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tweet                                                                                                                                                                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|üßµappalled by the academics RT‚Äôing India‚Äôs covid surge while still clueless about  India blocked protestors out of their own capitol &amp; will now argue that protestors are creating barriers to emergency response.  is responsible for farmers‚Äô &amp; COVID deaths.|\n",
      "|üßµappalled by the academics RT‚Äôing India‚Äôs covid surge while still clueless about  India blocked protestors out of their own capitol &amp; will now argue that protestors are creating barriers to emergency response.  is responsible for farmers‚Äô &amp; COVID deaths.|\n",
      "|üßµSome Data Around Agriculture. Read and take your own side in the ongoing \\nAnd if you have a counter view, show me Data, not emotions led tweets.                                                                                                                    |\n",
      "|üßµ1| We are delighted to learn from  in our upcoming event on Nov.20th! Sarover Zaidi is a philosopher &amp; social anthropologist who wrote this wonderful piece on the various speeds of the                                                                         |\n",
      "|üßµ ‚ÄúAmaranth is Chilai and Quinoa is Bathu‚Ä¶ I had to order it online. Amaranth at Rs 200 and Qunioa at 400, per kilo‚Ä¶ These plants grow all over Punjab and people treat them like weeds and  them.‚Äù \\n\\n                                                              |\n",
      "|üßµ Today I spoke on behalf of  in solidarity with  and  \\n\\nAs a male, urban tech worker, I'm not well connected or familiar with agricultural struggle which I want to change. \\n\\nüåç We are global, find a chapter!                                                  |\n",
      "|üßµ \\n\\nSupreme Court UPDATE :  üöúüåæ\\n\\n \\n\\n\\n \\n\\n                                                                                                                                                                                                                    |\n",
      "|üßµ\\nWhat a hypocritical world we're leaving in. As  is getting more attention day after other by various celebrities and politicians who are having vested interests in it. It is reported that around 200 farmers have LOST their life while protesting .             |\n",
      "|üß° \\n \\n \\n \\n \\n\\n\\nSee this  üëá\\n                                                                                                                                                                                                                                    |\n",
      "|üßê Of all people!                                                                                                                                                                                                                                                      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking changes.\n",
    "df_spark.select('tweet').orderBy('tweet', ascending=False).limit(10).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa6fce",
   "metadata": {},
   "source": [
    "Emails, links, tags, hashtags from The 'tweet' column values were successfully removed, however, from the above cell I can see that the  column 'tweet' still missing some cleaning. Further, I will remove leading and traling whitespaces, '&amp' character referencing for an ampersand, and non-English texts from the string value of the column. Then, I'll replace two or more continous whitespaces with a single whitespace, and also, I'll replace the new line and tab with '.'. And lastly I'll lowercase the entire string for each column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a3262b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading and trailing whitespace from the values of the column 'tweet'.\n",
    "df_spark = df_spark.withColumn('tweet', trim(df_spark.tweet))\n",
    "\n",
    "# Removing character referencing for an ampersand.\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', '&amp', ' '))\n",
    "\n",
    "# Replacing two or more spaces to one from the values of the column 'tweet'.\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', r'\\s{2,}', ' '))\n",
    "\n",
    "# Replacing new-line and tab to '.' from the values of the column 'tweet'.\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', r'\\n|\\t', '.'))\n",
    "\n",
    "# Removing non-English text from the values of the column 'tweet'.\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', \"[^a-zA-Z0-9!@#$%^&*()_+\\-={}\\[\\]|\\\\;:'\\\",.<>/?~` ]\", ''))\n",
    "\n",
    "# Lowercasing all the values of the column 'tweet'.\n",
    "df_spark = df_spark.withColumn('tweet', lower(df_spark.tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cd7ba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tweet                                                                                                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|yesterday in a public meeting alleged that leader staged managed a protest of farmer by burning his own crop field. today vaartha front page news covered in chandrasekar garu home district.now what you will say|\n",
      "|such a shame! i wish as well as take note of your tweet ; address the issue with corrective measures.                                                                                                             |\n",
      "|forty cents of every dollar that american farmers earn comes from the taxpayer. in the last two years us farm debt and bankruptcies have spiked as a result of trumps china and european trade policies.          |\n",
      "|has this been reported on msm? or are they ignoring this?                                                                                                                                                         |\n",
      "|.there seems to be a design to keep india in perpetual protest mode. why protest in india over an incident in                                                                                                     |\n",
      "|protesting on railway lines which is on railway land ; railways property, thus disrupting rail traffic has become a fashion. this is nonsense ; needs ban legally under law.                                      |\n",
      "|very irresponsible behaviour of govt of india with punjab they are not initiating for solving the i am from bjp and wishing defeat in bihar                                                                       |\n",
      "|given in and , the procurement season was advanced by a few days...                                                                                                                                               |\n",
      "|other side of apmc repeal: bihar farmers want mandis, like punjab | elections news,the indian express.                                                                                                            |\n",
      "|signature campaign launched by in against                                                                                                                                                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking changes.\n",
    "df_spark.select('tweet').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc64700",
   "metadata": {},
   "source": [
    "Ok, the cleaning is done and it looks fine. I'll safe this DataFrame to HDFS as CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd8de8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving the DataFrame to HDFS.\n",
    "df_spark.write.format('csv').save('hdfs://localhost:9000/ca2/tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ab444",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acbcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "432a19d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Converting my PyCharm DataFrame to Pandas DataFrame.\n",
    "df = df_spark.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b08fa2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1066380 entries, 0 to 1066379\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   date       1066380 non-null  object\n",
      " 1   user       1066380 non-null  object\n",
      " 2   tweet      1066380 non-null  object\n",
      " 3   replied    1066380 non-null  int32 \n",
      " 4   retweeted  1066380 non-null  int32 \n",
      " 5   liked      1066380 non-null  int32 \n",
      " 6   quoted     1066380 non-null  int32 \n",
      "dtypes: int32(4), object(3)\n",
      "memory usage: 40.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8d602e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>replied</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>liked</th>\n",
       "      <th>quoted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>OnTheFritz</td>\n",
       "      <td>forty cents of every dollar that american farm...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>ClaireDeLune</td>\n",
       "      <td>has this been reported on msm? or are they ign...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>Rajay Deep</td>\n",
       "      <td>such a shame! i wish as well as take note of y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          user   \n",
       "0  2020-11-01    OnTheFritz  \\\n",
       "1  2020-11-01  ClaireDeLune   \n",
       "2  2020-11-01    Rajay Deep   \n",
       "\n",
       "                                               tweet  replied  retweeted   \n",
       "0  forty cents of every dollar that american farm...        1          0  \\\n",
       "1  has this been reported on msm? or are they ign...        0          0   \n",
       "2  such a shame! i wish as well as take note of y...        0          0   \n",
       "\n",
       "   liked  quoted  \n",
       "0      3       0  \n",
       "1      1       0  \n",
       "2      2       0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f25ea3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>replied</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>liked</th>\n",
       "      <th>quoted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.066380e+06</td>\n",
       "      <td>1.066380e+06</td>\n",
       "      <td>1.066380e+06</td>\n",
       "      <td>1.066380e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.270138e-01</td>\n",
       "      <td>7.838468e+00</td>\n",
       "      <td>1.730490e+01</td>\n",
       "      <td>6.012163e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.593291e+02</td>\n",
       "      <td>3.187909e+02</td>\n",
       "      <td>9.662562e+02</td>\n",
       "      <td>4.602977e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.630650e+05</td>\n",
       "      <td>3.155470e+05</td>\n",
       "      <td>9.443070e+05</td>\n",
       "      <td>4.583200e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            replied     retweeted         liked        quoted\n",
       "count  1.066380e+06  1.066380e+06  1.066380e+06  1.066380e+06\n",
       "mean   9.270138e-01  7.838468e+00  1.730490e+01  6.012163e-01\n",
       "std    1.593291e+02  3.187909e+02  9.662562e+02  4.602977e+01\n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
       "50%    0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00\n",
       "75%    0.000000e+00  2.000000e+00  3.000000e+00  0.000000e+00\n",
       "max    1.630650e+05  3.155470e+05  9.443070e+05  4.583200e+04"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8728cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac4aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad807c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dae84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3629a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918f34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8a90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62cacea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                user|count|\n",
      "+--------------------+-----+\n",
      "|  The Pacifist Rebel| 7741|\n",
      "|       Kisan Botüöúüåæ| 7338|\n",
      "| ‡®∏‡®∞‡®¶‡®æ‡®∞ ‡®∏‡®æ‡®¨(A Farmer)| 4403|\n",
      "|‡®ï‡®ø‡®∏‡®æ‡®®Andolajivi r...| 4107|\n",
      "|‡®ï‡®ø‡®∏‡®æ‡®®Andolajivi R...| 4107|\n",
      "|             Navneet| 3706|\n",
      "|       Navneet Jammu| 3659|\n",
      "|          IndiaToday| 3480|\n",
      "|           üçäramanüöú| 3251|\n",
      "|PARSHOTAM SINGH S...| 3210|\n",
      "|                 Jot| 3133|\n",
      "|Jaz üá®üá¶üåæ ‡®ó‡®∞‡®Æ ‡®ñ‡®ø‡®Ü‡®≤‡©Ä| 3052|\n",
      "|     Manminder Singh| 2561|\n",
      "|For love of Punja...| 2495|\n",
      "|Jaspal Kaur Bains...| 2495|\n",
      "|                John| 2489|\n",
      "|‡®§‡®ï‡®¶‡©Ä‡®∞ ‡®ï‡©å‡®∞ üá∫üá∏ ‡®¶‡©Å‡®Ü‡®¨‡®£| 2480|\n",
      "|    Shivinder Thakur| 2421|\n",
      "|    Kuldeep Dhaliwal| 2405|\n",
      "|           onnniiaaa| 2382|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 70:>                                                         (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('user').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.collect()[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7007f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c84e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_spark.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from tweet').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d4922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
