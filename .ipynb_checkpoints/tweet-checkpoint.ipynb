{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d04671",
   "metadata": {},
   "source": [
    "                                                                       TEMUULEN Bulgan - 2022427"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf25cb4",
   "metadata": {},
   "source": [
    "### CCT College Dublin Continuous Assessment No.2\n",
    "# AN ANALYSIS OF INDIAN FARMERS' PROTEST TWEETS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad4ca0",
   "metadata": {},
   "source": [
    "#### Brief Introduction of the project:\n",
    "\n",
    "1. For my second continuous assessment, I choose CSV format data of Indian Farmer's Protest Tweets. This file contains over 1 million English language tweets tweeted between November 1st, 2020 and november 21st, 2021 with the hashtag <#FarmersProtest>. It is downloaded from the Kaggle website with the CCO:Public Domain license.\n",
    "(https://kaggle.com/datasets/prathamsharma123/farmers-protest-tweets-dataset-csv)\n",
    "\n",
    "2. I divided my project into 3 primary sections (Every step in data processing and analysis is fully discussed on each subsection of these primary sections.):\n",
    "    1. big data storage and processing\n",
    "    2. sentiment analysis \n",
    "    3. EDA and forecast\n",
    "    4. databases and comparison\n",
    "\n",
    "3. I used Git for daily code tracking and GitHub for archiving, monitoring and sharing. To view the whole project on GitHub, click on the follwig link "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3b7ce",
   "metadata": {},
   "source": [
    "#### Libraries and modules used for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de876cf9",
   "metadata": {},
   "source": [
    "## 1. Big Data Storage and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80edbb",
   "metadata": {},
   "source": [
    "For my second continous project I choose Kaggle.com's dataset called 'Farmers Protest Tweets'. It was collected by the hashtag #FarmersProtest including CCO:Pulic Domain license which means that this dataset allows copy, modify, distribute and perform the work, even for commercial purpososes, all without asking permission. All the tweets in it in English Language and collected from Twitter.com from November 1st, 2020 to November 21st, 2021. The main subject matter of these tweets are about the biggest anti-farm laws protest which took place at the borders of the Indian natioanl capital of New Delhi, organized by coalition of over 40 farmer from across the country.The dataset extraction process was done by Pratham Sharma, kaggle datasets expert who used Twitter API and  snscrape Python library for collection. The tweets data consist of two separate CSVL files with size the size of 1.7Â GB and 81.2 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47471a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the file size of the file.\n",
    "file_size = os.path.getsize('/home/hduser/Desktop/ca/tweets.csv')/(1024*1024*1024)\n",
    "print(f'The size of the tweets file is: {file_size:.2f} GB.')\n",
    "file_size = os.path.getsize('/home/hduser/Desktop/ca/users.csv')/(1024*1024)\n",
    "print(f'The size of the users file is: {file_size:.2f} MB.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b503dd",
   "metadata": {},
   "source": [
    "### 1.1. *Preprocessing in Python.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83926380",
   "metadata": {},
   "source": [
    "Before to start processing the DataSet in distributed file system platforms I decided to examine each CSV file using Pandas on Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53351e86",
   "metadata": {},
   "source": [
    "#### *tweets.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9151b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame.\n",
    "df_tweets = pd.read_csv('/home/hduser/Desktop/ca/tweets.csv')\n",
    "\n",
    "# Checking the DataFrame.\n",
    "df_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fc0c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Printing information about the DataFrame\n",
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63926d09",
   "metadata": {},
   "source": [
    "#### *users.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame.\n",
    "df_users = pd.read_csv('/home/hduser/Desktop/ca/users.csv')\n",
    "\n",
    "# Checking the DataFrame.\n",
    "df_users.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing information about the DataFrame\n",
    "df_users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dfac9",
   "metadata": {},
   "source": [
    "As we can see from abowe first two code sells the DataSet with tweets consisted of 13 distinct columns of information where some of which is not very importand for further processing. The columns such as 'tweetUrl', 'tweetId', 'source', 'media', 'retweetedTweet', 'quotedTweet', 'mentionedUsers', and 'userId' doesn't include importand information for analysis.\n",
    "\n",
    "On the other hand last two code sells show that the DataSet with users information consisted of 18 distincst columns of information from which I can only use only column with 'display name. So further I'm going to remove unnessecary columns from each DataFrame and merge them as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac869a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deleting columns from the DataFrame of tweets.\n",
    "df_tweets = df_tweets.drop(labels=['tweetUrl', 'tweetId', 'source', 'media', 'retweetedTweet', 'quotedTweet', \\\n",
    "                                   'mentionedUsers'], axis=1)\n",
    "\n",
    "# Checking the changes.\n",
    "df_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting columns from the DataFrame of tweets.\n",
    "df_users = df_users[['displayname', 'userId']]\n",
    "\n",
    "# Checking the changes.\n",
    "df_users.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026b755",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Performing left merging on two DataFrames.\n",
    "df_final = pd.merge(df_tweets, df_users, on='userId', how='left')\n",
    "\n",
    "# Deleting the column 'userId'.\n",
    "df_final = df_final.drop(labels=['userId'], axis=1)\n",
    "\n",
    "# Checking the changes.\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823631b",
   "metadata": {},
   "source": [
    "I merged two DataFrames and removed unessary columns. Now I'm going to save the it as a CSV file on my Ubuntu VM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as 'new_tweets.csv' on my VM with the utf-8 Encoding.\n",
    "df_final.to_csv('/home/hduser/Desktop/ca/new_tweets.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ce26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the file size.\n",
    "new_file_size = os.path.getsize('/home/hduser/Desktop/ca/new_tweets.csv')/(1024*1024)\n",
    "print(f'The size of new tweet file is: {new_file_size:.2f} MB.')\n",
    "\n",
    "print(df_final.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11069657",
   "metadata": {},
   "source": [
    "The file size is reduced from 1.7GB to 670.2MB and it still have importand information of the tweets for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5681470",
   "metadata": {},
   "source": [
    "### 1.2 *Data cleaning in Pyspark.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058322bc",
   "metadata": {},
   "source": [
    "I desided to store my new created twitter dataset file in HDFS and before doing EDA and sentiment analysis I'll further do more thorough preprocessing and cleaning done using Apache Spark tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babcb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a new SparkSession for data import from HDFS.\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Sentiment Analysis') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896a58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading the file.\n",
    "df_spark = spark.read.option('header', 'true') \\\n",
    "                        .option('multiline', 'true') \\\n",
    "                        .option('quote', \"\\\"\") \\\n",
    "                        .option('escape', \"\\\"\") \\\n",
    "                        .csv('/ca2/new_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35ed81",
   "metadata": {},
   "source": [
    "Using 'SparkSession.builder' I created a new SparkSession for interacting with Spark functionality. Then I created a new PySpark DataFrame by importing the 'new_tweets.csv' file which is stored in HDFS's 'ca2' directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b878c8",
   "metadata": {},
   "source": [
    "#### *columns*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e051a",
   "metadata": {},
   "source": [
    "First thing I would like to do is check all the columns and its datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542642c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the contents of the DataFrame.\n",
    "df_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013bd5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the columns of the DataFrame\n",
    "x = 0\n",
    "columns = ''\n",
    "while x < len(df_spark.columns):\n",
    "    columns += df_spark.columns[x] + ', '\n",
    "    x += 1\n",
    "print('Columns of the DataFrame are:', columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e4a80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking column dtypes.\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59343b19",
   "metadata": {},
   "source": [
    "I examined the DataFrame with its columns and found that because it was imported from the CSV file all 7 columns contained the string dtype values. I'll change the names of the columns to make it easy to use, and dtypes of the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f8b89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Renaming the columns.\n",
    "df_spark = df_spark.select(col('date').alias('date'),\n",
    "                           col('displayname').alias('user'),\n",
    "                           col('renderedContent').alias('tweet'),\n",
    "                           col('replyCount').alias('replied'),\n",
    "                           col('retweetCount').alias('retweeted'),\n",
    "                           col('likeCount').alias('liked'),\n",
    "                           col('quoteCount').alias('quoted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing dtypes.\n",
    "df_spark = df_spark.withColumn('date',to_timestamp(col('date').cast(TimestampType())))\n",
    "df_spark = df_spark.withColumn('tweet',col('tweet').cast(StringType()))\n",
    "df_spark = df_spark.withColumn('user',col('user').cast(StringType()))\n",
    "df_spark = df_spark.withColumn('replied',col('replied').cast('integer'))\n",
    "df_spark = df_spark.withColumn('retweeted',col('retweeted').cast('integer'))\n",
    "df_spark = df_spark.withColumn('liked',col('liked').cast('integer'))\n",
    "df_spark = df_spark.withColumn('quoted',col('quoted').cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2895340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking changes.\n",
    "df_spark.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35c43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking changes.\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33693c",
   "metadata": {},
   "source": [
    "Now all the column values has the appropirate data format as well as the names of the columns are short and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3240dd",
   "metadata": {},
   "source": [
    "#### *null values and duplicates*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab113e2",
   "metadata": {},
   "source": [
    "Second thing I would like to do for cleaning is removing the duplicates and null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609dac0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Examining the shape of the DataFrame\n",
    "print('The DataFrame consists of', len(df_spark.columns), 'columns and', df_spark.count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62bba6",
   "metadata": {},
   "source": [
    "The Dataframe consists of 2.953.850 rows and 7 colums. And as can be seen from the 'show()' method above it is obvious that there are lots of duplicates on it, so I would like to remove them. \n",
    "\n",
    "Moreover, there are some NaN values in the new merged column 'displayname'. It seems like some users that tweeted on the twitter doesn't have a Display Name. It might indicate that they no longer user of the social media plaform, or even might have blocked becuse of trolling or whatever reason, the account might have been deleted. So to prevent the bias and also to not waste my memory for extra processing I have decided to drop the rows which doesn't contain the display name or contain duplicate tweet entries.\n",
    "\n",
    "After these changes I'll order table contents by the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicates and rows with null values then ordering the rows.\n",
    "df_spark = df_spark.drop_duplicates()\n",
    "df_spark = df_spark.filter(col('displayname').isNotNull())\n",
    "df_spark = df_spark.orderBy('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the changes.\n",
    "df_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7885c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the changes.\n",
    "print('The DataFrame consists of', len(df_spark.columns), 'columns and', df_spark.count(), 'rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values on entire DataFrame.\n",
    "df_spark.filter(col('date').isNull() | col('user').isNull() | col('tweet').isNull() | col('replied').isNull() | \\\n",
    "                col('retweeted').isNull() | col('liked').isNull() | col('quoted').isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf7bd2",
   "metadata": {},
   "source": [
    "Above executed cells show that now the DataFrame shape is 1.066.380x7. There are no dublicates and no null values, it is now clean and clear. However since I'll do the sentimental analysis for this project I would like to clean the entries of the column 'tweet'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf511d2",
   "metadata": {},
   "source": [
    "#### *the column 'tweet'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237433f2",
   "metadata": {},
   "source": [
    "Firstly I will remove tags, hashtags, emails, links from the tweets because I would like to focus on the text itself, rather than considering the additional context. They extra contexts are not  directly contribute to the sentiment of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cd9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing tags, hashtags, emails, and website links from the values of the column 'tweet'.\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', r'@\\w+|#\\S+|\\S+@\\S+|http\\S+|www\\S+|\\S+/\\S+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b39ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking changes.\n",
    "df_spark.select('tweet').orderBy('tweet', ascending=False).limit(10).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92349b01",
   "metadata": {},
   "source": [
    "Emails, links, tags, hashtags from The 'tweet' column values were successfully removed, however, from the above cell I can see that the  column 'tweet' still missing some cleaning. Further, I will remove leading and traling whitespaces, '&amp' character referencing for an ampersand, punctuation marks and non-English texts from the string value of the column. Then, I'll replace two or more continous whitespaces with a single whitespace, and also, I'll replace the new line and tab with ' '. And lastly I'll lowercase the entire string for each column value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55908d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra cleaning for the 'tweet' column values\n",
    "df_spark = df_spark.withColumn('tweet', trim(df_spark.tweet))\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', '&amp', ' '))\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', '[\\|,.;:\\?!_+-]', ' '))\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', r'\\s{2,}', ' '))\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', r'\\n|\\t', ' '))\n",
    "df_spark = df_spark.withColumn('tweet', regexp_replace('tweet', \"[^a-zA-Z0-9!@#$%^&*()_+\\-={}\\[\\]|\\\\;:'\\\",.<>/?~` ]\", ''))\n",
    "df_spark = df_spark.withColumn('tweet', lower(df_spark.tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617ce8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking changes.\n",
    "df_spark.select('tweet').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b38456",
   "metadata": {},
   "source": [
    "Ok, the cleaning is done and it looks fine. I'll safe this DataFrame to HDFS as CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3fb06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame to HDFS.\n",
    "df_spark.write.format('csv').save('hdfs://localhost:9000/ca2/tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a05826",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37fab42",
   "metadata": {},
   "source": [
    "Since I'm using tweet DataSet about the protest it is obvious taht the most tweets would have negative tone of writing. And instead of focusing solely on positive or negative sentiments I decided to use lexicon-based emotion analysis approach for my Sentiment Analysis which is more nuanced analysis approach at the sense level. I choose NRC-Emotion-Lexicon-Senselevel-v0.92 which associates emotions with specific word senses of or meaning. It includes emotions such as anger, anticipation, disgust, fear, joy, sadness, surprice and trust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9274b91",
   "metadata": {},
   "source": [
    "### 2.1 *Lexicon File*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27f079",
   "metadata": {},
   "source": [
    "I downloaded lexicon file from the website mentioned on the reference part of the report and uploaded it into HDFS. And now using PySpark's read.csv function I'm going to create the PySpark DataFrame of the lexicon file for emotions sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing csv file from HDFS.\n",
    "df_lexicon = spark.read.csv('/ca2/NRC-Emotion-Lexicon-Senselevel-v0.92.txt', sep='\\t', header=False, inferSchema=True)\n",
    "\n",
    "# Checking the ne created DataFrame.\n",
    "df_lexicon.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39c03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examining the shape of the DataFrame\n",
    "print('The DataFrame consists of', len(df_lexicon.columns), 'columns and', df_lexicon.count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d4c96",
   "metadata": {},
   "source": [
    "I have now lexicon DataFrame wich consists of 3 columns and 241590 rows. Next step is I'm going to crate a separate DataFrame with column of tweets since the 'tweet' is the only column that I'm gonna use for the sentiment Analysis. However I'm not using this column separately alone because of the PySpark's parallel processing and data partitioning the row order will change and I won't be able to do the further forecasting in the result. To keep the order for the row data i will use column 'date'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf052c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a new DataFrame for sentiment analysis.\n",
    "df_for_sentiment = df_spark.select('date', 'tweet')\n",
    "\n",
    "# Checking the DataFrame.\n",
    "df_for_sentiment.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636615fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Examining the shape of the DataFrame.\n",
    "print('The DataFrame consists of', len(df_for_sentiment.columns), 'columns and', df_for_sentiment.count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d42e93",
   "metadata": {},
   "source": [
    "### 2.2 *Tokenization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6303d",
   "metadata": {},
   "source": [
    "I've separated and created a new DataFrame called 'df_for_sentiment' it includes columns 'date' and 'tweet'. Next step is tokenizing, using the default 'Tokenizer' class from the 'pyspark.ml.feature' module, which will split text into individual tokens based on default delimiter 'whitespace'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b026c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting tweet strings into tokenized text.\n",
    "tokenizer = Tokenizer(inputCol='tweet', outputCol='token')\n",
    "df_for_sentiment = tokenizer.transform(df_for_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7756073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the changes.\n",
    "df_for_sentiment.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb52faf",
   "metadata": {},
   "source": [
    "### *2.3 Stop word removal*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb58da",
   "metadata": {},
   "source": [
    "I have now a new column 'token' with tokenized text on it, and when I look closer to the text in it there are lots of stop words that are not very inportand for further processing. Since they do not carry sifnificant meaning or contribute to the sentiment of a text, by removing them I will reduce noise and focus on the more meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words from token text.\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
    "remover = StopWordsRemover(inputCol='token', outputCol='clean_token', stopWords=stopwords)\n",
    "df_for_sentiment = remover.transform(df_for_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547d648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the changes.\n",
    "df_for_sentiment.select('clean_token').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c961cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking removed stop words list for PySpark.ML.\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd63166",
   "metadata": {},
   "source": [
    "### *2.4 Lemmatization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea086d",
   "metadata": {},
   "source": [
    "As it can bee seen from above subsection I have removed default stop words from the token text. And now I would like to do the lemmatizzation to reduce words to their base or root form. It will help me normalize words and reduce the dimentionality of each row.\n",
    "\n",
    "I tried lots of different libraries which has Lemmatization tool in it however all of them didn't work with PySpark. There were some errors in execution with Javascript, some had library import error, some libraries stopped development of the tool and etc. So that I decided to convert my PySpark DataFrame to Pandas DataFrame and use NLTK for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the DataFrame\n",
    "df_for_sentiment.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new DataFrame for Pandas\n",
    "df_for_pandas = df_for_sentiment.select('date', 'clean_token')\n",
    "\n",
    "# Converting the values of the 'date' column to strings.\n",
    "df_for_pandas = df_for_pandas.withColumn('date_string', col('date').cast('string'))\n",
    "\n",
    "# Using 'clean_token' and 'date_string' for Pandas.\n",
    "df_for_pandas = df_for_pandas.select('clean_token', 'date_string')\n",
    "\n",
    "# Conversion from Pyspark to Pandas\n",
    "df_pandas = df_for_pandas.toPandas()\n",
    "\n",
    "# Checking the changes\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ea1ce",
   "metadata": {},
   "source": [
    "I'm trying to keep the column with dates together with the column of tweets because if i process tweets separatily it will loose the order that I'm trying to keep for further processing. Now we have a Pandas DataFrame and I'll do lemmatazation here then I'll convert it back again to Pyspark DataFrame, since I choose to do the sentiment analysis in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be31ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d59f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43756018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0539184",
   "metadata": {},
   "source": [
    "### 2.1 *Loading the lexicon*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221f17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80026f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a55419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9fa91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e3f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaed04a9",
   "metadata": {},
   "source": [
    "## 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9da6e0",
   "metadata": {},
   "source": [
    "Since the file size of new tweets dataset is not too large and Pandas provides more user-friendly and interactive environment for Exploratory Data Analysis, I decided to do the EDA on Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting my PyCharm DataFrame to Pandas DataFrame.\n",
    "df = df_spark.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the size of dataset in terms of memory usage.\n",
    "memory_usage = df.memory_usage(deep=True).sum()/(1024*1024)\n",
    "print(f'The Pandas DataFrame size is: {memory_usage:.2f} MB.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fa2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4490b",
   "metadata": {},
   "source": [
    "The Dataset was succefully converted to Pandas and the size of memory usage is 306.74. Everythings seems okay for analysis, only thing do to is change the dtype of the column 'date' to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c084169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ea3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef537912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537cd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72abfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432ac52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e2bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81b30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346253e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cacea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_spark.groupBy('user').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.collect()[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7007f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c84e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_spark.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from tweet').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d4922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
